\documentclass[a4paper,12pt]{article}

\usepackage[top = 2.5cm, bottom = 2.5cm, left = 2.5cm, right = 2.5cm]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{booktabs,amsmath,amsthm}
\usepackage{changepage}
\usepackage{graphicx} 
\usepackage{setspace}
\usepackage{amsthm}
\usepackage{comment}
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{pgfplots}
\usepackage{ulem}

\setlength{\parindent}{0in}
\pagestyle{fancy} 
\fancyhf{}
\pgfplotsset{compat=1.18}

\newtheoremstyle{nonitalic}
  {3pt}
  {3pt}
  {\normalfont}
  {}
  {\bfseries}
  {.}
  {.5em}
  {}

\theoremstyle{nonitalic}

\newtheorem{definition}{Definition}[subsection]

\newtheorem{theorem}{Theorem}[subsection]

\lhead{\footnotesize STA260 Notes}
\rhead{\footnotesize sytez} 
\cfoot{\footnotesize \thepage}

\begin{document}
    \thispagestyle{empty}

    \begin{tabular}{p{15.5cm}}  
        {\Large \bf STA260 Notes}\\
        By sytez\\
        \hline
    \end{tabular}

    \begin{center}
        {\Large \bf Preface}
    \end{center}

    \quad These notes were made for the course STA260: Probability and Statistics II at the University of Toronto Mississauga. They are based on the lectures and lecture slides of the 2024 summer offering. This course was instructed by Professor Luai Al Labadi (The Goat). These notes primarily consist of a summary of the defintions and theorems from the course. The notes are not exhaustive and may contain errors. The proofs provided are not complete and instead provide a brief outline of the proof containing the main ideas. Theorems that do not contain a proof are usually trivial to prove with exceptions of the ones whose proof is beyond the scope of this course.

    \newpage
    
    \tableofcontents
    \newpage

    \setcounter{section}{6}

    \section{Chapter 7 \textemdash{} Sampling Distributions and the Central Limit Theorem}

    \subsection{Introduction}

    \begin{definition}
        A \textbf{population} consists of the entire collection of the observations with which we are concerned.
    \end{definition}

    \bigskip

    \begin{definition}
        A \textbf{sample} is a subset of a population.
    \end{definition}

    \bigskip

    \begin{definition}
        A \textbf{parameter} is a numerical summary of a population. For example, the mean, the variance, etc. In practice, it is unknown.
    \end{definition}

    \bigskip

    \begin{definition}
        A \textbf{statistic} is a numerical summary of a sample. For example, the sample mean, the sample variance, etc.
    \end{definition}

    \bigskip

    \begin{definition}
        The statistic varies from sample to sample and hence it is a random variable and has a probability distribution called the \textbf{sampling distribution}.
    \end{definition}

    \bigskip

    The knowledge of the sampling distribution of a statistic helps to make an inference about the corresponding population (true) parameter.

    \bigskip

    \begin{definition}
        We say that the random variables $Y_1, Y_2, \ldots, Y_n$ are \textbf{independent and identically distributed} (i.i.d.) if they are independent random variables and have the same probability distribution (same pdf/cdf).
    \end{definition}

    \bigskip

    For a random sample, we write
    \[
        Y_1, \ldots, Y_n \overset{\text{i.i.d.}}{\sim} f(y) \quad \text{(continuous)}
    \]
    \[
        Y_1, \ldots, Y_n \overset{\text{i.i.d.}}{\sim} p(y) \quad \text{(discrete)}
    \]
    \newpage

    \subsection{Sampling Distributions}
    
    \begin{definition}
        The \textbf{sample mean} is defined as
        \[
            \overline{Y} = \frac{1}{n} \sum_{i=1}^{n} Y_i
        \]
    \end{definition}

    \begin{definition}
        The \textbf{sample variance} is defined as
        \[
            S^2 = \frac{1}{n-1} \sum_{i=1}^{n} (Y_i - \overline{Y})^2
        \]
    \end{definition}

    \begin{theorem}
        Let $Y_1, \ldots, Y_n$ be an iid random sample from a population with any distribution with finite mean $\mu$ and finite variance $\sigma^2$. Then \(E(\overline{Y}) = \mu\) and \(V(\overline{Y}) = \frac{\sigma^2}{n}\).
    \end{theorem}

    \begin{theorem}
        If $Y_1, \ldots, Y_n \overset{\text{i.i.d.}}{\sim} N\left(\mu, \sigma^2\right)$, then
        \[
        \overline{Y} \sim N\left(\mu, \frac{\sigma^2}{n}\right)
        \]
    \end{theorem}
    
    
    \begin{proof}
        Show MGF of $\overline{Y}$ is the MGF of $N\left(\mu, \frac{\sigma^2}{n}\right)$ then conclude by uniqueness of MGF.
    \end{proof}
    
    \begin{definition}
        The \textbf{standard normal distribution} is defined as
        \[
            Z = \frac{Y - \mu}{\sigma} = \frac{\overline{Y} - \mu}{\sigma/\sqrt{n}} = N(0,1)
        \]
    \end{definition}

    \begin{theorem}
        Let \( Y_1, \ldots, Y_n \overset{\text{i.i.d.}}{\sim} N(\mu, \sigma^2) \). If \( Z_i = \frac{Y_i - \mu}{\sigma} \), then
        \[
        \sum_{i=1}^{n} Z_i^2 = \sum_{i=1}^{n} \left( \frac{Y_i - \mu}{\sigma} \right)^2
        \]
        has a \(\chi^2\) distribution with \(n\) degrees of freedom (df)
    \end{theorem}
    
    \begin{proof}
        Note 3 facts:
        \begin{enumerate}
            \item \(\chi^2_{(v)} = \text{Gamma}(v/2, 2)\)
            \item \(\chi^2_{(v_1)} + \chi^2_{(v_2)} \sim \chi^2_{(v_1 + v_2)}\) assuming independence
            \item \([N(0,1)]^2 = Z^2 \sim \chi^2_{(1)}\)
        \end{enumerate}

        The proof becomes trivial from here.
    \end{proof}

    Note that similarly we have the sum of many distributions can be studied easily.
    \[\begin{aligned}
        &Y_1, \ldots, Y_n \overset{\text{iid}}{\sim} \text{Ber}(p) \implies \sum_{i=1}^{n} Y_i \sim \text{Bin}(n, p) \\
        &Y_1, \ldots, Y_n \overset{\text{iid}}{\sim} \text{Poisson}(\lambda) \implies \sum_{i=1}^{n} Y_i \sim \text{Poisson}(n\lambda) \\
        &Y_1, \ldots, Y_n \overset{\text{iid}}{\sim} \chi^2(v_i) \implies \sum_{i=1}^{n} Y_i \sim \chi^2_{\left(\sum\limits_{i=1}^{n} v_i\right)}\\
        &Y_1, \ldots, Y_n \overset{\text{iid}}{\sim} N(\mu_i, \sigma^2_i) \implies \sum_{i=1}^{n} Y_i \sim N\left(\sum\limits_{i=1}^{n} \mu_i, \sum\limits_{i=1}^{n} \sigma^2_i\right)
    \end{aligned}\]

    \begin{proof}
        Note the following fact:\\
        Let $U = Y_1 + \ldots + Y_n$ then we have $M_U(t) = M_{Y_1}(t) \cdot \ldots \cdot M_{Y_n}(t)$.\\
        That is that the MGF of the sum of random variables is the product of the MGFs of the random variables.\\
        The proof follows trivially using the unqiueness of MGFs.
    \end{proof}

    \begin{theorem}
        As a corollary of theorem 7.2.2\\
        If \( Y_1, \ldots, Y_n \overset{\text{i.i.d.}}{\sim} N(\mu, \sigma^2) \), then
        \[
            U = \sum_{i=1}^{n} \left( \frac{Y_i - \mu}{\sigma} \right)^2 \sim \chi^2_{(n)}
        \]
        similarly one sees that if \( Y_i \sim N(\mu_i, \sigma_i^2), \, i = 1, \ldots, n \) (and independent), then
        \[
        U = \sum_{i=1}^{n} \left( \frac{Y_i - \mu_i}{\sigma_i} \right)^2 \sim \chi^2(n).
        \]
    \end{theorem}

    \begin{theorem}
        Let \( Y_1, Y_2, \ldots, Y_n \) be a random sample from a normal distribution with mean \(\mu\) and variance \(\sigma^2\). Then
        \[
        \frac{(n-1)S^2}{\sigma^2} = \sum_{i=1}^{n} \frac{(Y_i - \overline{Y})^2}{\sigma^2} \sim \chi^2_{n-1}.
        \]
        Additionally, \(\overline{Y}\) and \(S^2\) are independent.
    \end{theorem}

    \newpage

    \subsection{The t-Distribution}

    \begin{definition}
        Let $Z \sim N(0,1)$ and $W \sim \chi^2_{(v)}$. Then if $Z$ and $W$ are independent then we say
        \[
            T = \frac{Z}{\sqrt{W/v}} \sim t_{(v)}
        \]
        has a \textbf{$t$-Distribution} with $v$ degrees of freedom.
    \end{definition}

    So far it was assumed that the population standard deviation $\sigma$ is known. However, this assumption may be unreasonable. We want to estimate both $\mu$ and $\sigma$. A natural statistic to deal with inferences on $\mu$ is
    \[
        T = \frac{\overline{Y} - \mu}{S/\sqrt{n}} \sim t_{(n-1)}
    \]
    
    \textbf{Properties of the $t$-Distribution}
    
    \begin{figure}[h!]
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                axis lines=middle,
                xlabel={$0$},
                ylabel={},
                xtick=\empty,
                ytick=\empty,
                ymin=0,
                xmin=-4, xmax=4,
                samples=100,
                height=5cm,
                width=8cm,
                enlarge x limits=false,
                enlarge y limits=false,
                clip=false,
                ]
                % Standard Normal Curve
                \addplot[thick, black, domain=-4:4] {1/sqrt(2*pi) * exp(-0.5*x^2)} node[pos=0.8, pin={[pin distance=3cm]150:Standard Normal}] {};
                % t-distribution Curve (with 10 degrees of freedom)
                % (gamma(11/2)/(sqrt(10*pi)*gamma(5))) = 0.389108383
                \addplot[thick, blue, domain=-4:4] {0.389108383 * (1 + x^2/10)^(-11/2)} node[pos=0.7, pin={[pin distance=2cm]95:$t_{(10)}$}] {};
                % t-distribution Curve (with 5 degrees of freedom)
                % (gamma(3)/(sqrt(5*pi)*gamma(5/2))) = 0.379606689
                \addplot[thick, red, domain=-4:4] {0.379606689 * (1 + x^2/10)^(-11/2)} node[pos=0.7, pin={[pin distance=2cm]80:$t_{(5)}$}] {};
            \end{axis}
        \end{tikzpicture}
        \caption{Comparison of Standard Normal and $t$-distribution curves with 5 and 10 degrees of freedom}
    \end{figure}

    \begin{itemize}
        \item Each $t_\nu$ curve is bell-shaped and centered at 0.
        \item Each $t_\nu$ curve is spread out more than the standard normal ($Z$) curve.
        \item As $\nu$ increases, the spread of the corresponding $t_\nu$ curve decreases.
        \item As $\nu \to \infty$, the sequence of $t_\nu$ curves approaches the standard normal curve (the $Z$ curve is called a $t$ curve with df $=\infty$).
    \end{itemize}

    \newpage

    \subsection{The F-Distribution}

    \begin{definition}
        Let \( W_1 \sim \chi^2_{(\nu_1)} \) and \( W_2 \sim \chi^2_{(\nu_2)} \) be independent. Then
        \[
            F = \frac{W_1 / \nu_1}{W_2 / \nu_2}
        \]
        has a \textbf{\( F \)-Distribution} with \( \nu_1 \) and \( \nu_2 \) degrees of freedom.
    \end{definition}

    \begin{theorem}
        If \( S_1^2 \) and \( S_2^2 \) are the variances of independent random samples of size \( n_1 \) and \( n_2 \) taken from normal populations with variances \( \sigma_1^2 \) and \( \sigma_2^2 \), respectively, then
        \[
        \frac{S_1^2 / \sigma_1^2}{S_2^2 / \sigma_2^2} \sim F(n_1 - 1, n_2 - 1).
        \]        
    \end{theorem}

    \begin{proof}
        Recall that
        \[
            \frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{(n-1)}
        \]
        the proof trivially follows.
    \end{proof}
    
    \begin{theorem}
        Let $Y$ be a random variable such that $Y \sim F_{(v_1, v_2)}$. Then
        \[
            \frac{1}{Y} \sim F_{(v_2, v_1)}
        \]
    \end{theorem}

    \begin{theorem}
        Let $Y_1 \sim F_{(v_1, v_2)}$ then
        \[
            \left(1 + \frac{v_1}{v_2}Y_1\right)^{-1} \sim \text{Beta}\left(\frac{v_2}{2}, \frac{v_1}{2}\right)
        \]
    \end{theorem}
    
    \begin{proof}
        Recall that
        \[
            \chi^2_{(v)} = \text{Gamma}\left(\frac{v}{2}, 2\right) \quad \text{and} \quad \frac{\text{Gamma}(\alpha, \gamma)}{\text{Gamma}(\alpha, \gamma) + \text{Gamma}(\beta, \gamma)} = \text{Beta}(\alpha, \beta)
        \]
        the proof follows trivially using the defintion of the $F$-distribution.
    \end{proof}

    \begin{theorem}
        Let $T$ be a random variable with a $t$-distribution with $v$ degrees of freedom. Then $U = T^2$ has an $F$-distribution with $1$ and $v$ degrees of freedom.
    \end{theorem}

    \newpage

    \subsection{The Central Limit Theorem}

    We already showed that if \(Y_1, Y_2, \ldots, Y_n\) represents a random sample from any distribution with mean \(\mu\) and variance \(\sigma^2\), then \(E(\overline{Y}) = \mu\) and \(V(\overline{Y}) = \frac{\sigma^2}{n}\).\\
    In what follows, we will develop an approximation for the sampling distribution of \(\overline{Y}\) that can be used \uline{regardless of the distribution of the population from which the sample is taken}.

    \bigskip

    \begin{theorem} \textbf{Central Limit Theorem}\\
        Let \(Y_1, Y_2, \cdots, Y_n\) be a random sample from a population with finite mean \(\mu\) and finite variance \(\sigma^2\), but unknown distribution. Then if \(n\) is sufficiently large, \(\overline{Y}\) is \textcolor{red}{approximately normally distributed} with mean \(\mu\) and variance \(\frac{\sigma^2}{n}\), i.e. $\overline{Y} \approx N(\mu, \frac{\sigma^2}{n})$.
    \end{theorem}

    \bigskip

    The CLT can be written more formally as:\\
    If
    \[
        U_n = \frac{\sum_{i=1}^n Y_i - n\mu}{\sigma \sqrt{n}} = \frac{\overline{Y} - \mu}{\sigma / \sqrt{n}}
    \]
    then
    \[
        U_n \overset{d}{\to} N(0, 1)
    \]

    This implies that
    \[
        \lim_{n \to \infty} P(U_n \leq u) = \int_{-\infty}^{u} \frac{1}{\sqrt{2\pi}} e^{-t^2/2} \, dt
    \]

    \begin{proof}
        Let $U_n = \frac{\overline{Y} - \mu}{\sigma / \sqrt{n}}$, now we can rewrite $U_n = \frac{1}{\sqrt{n}} \sum\limits_{i=1}^{n} Z_i$.\\
        Now note that
        \[
            M_{U_n}(t) = M_{\frac{1}{\sqrt{n}} \sum\limits_{i=1}^{n} Z_i}(t) = M_{\sum\limits_{i=1}^{n} Z_i}\left(\frac{t}{\sqrt n}\right) = \prod_{i=1}^{n} M_{Z_i} \left(\frac{t}{\sqrt n}\right) = \left[M_{Z_1}\left(\frac{t}{\sqrt{n}}\right)\right]^n
        \]
        Now through the use of the McLaurin expansion of \(M_{Z_1}\left(\frac{t}{\sqrt{n}}\right)\) it can be shown that
        \[
            \lim_{n \to \infty} n \cdot M_{Z_1}\left(\frac{t}{\sqrt{n}}\right) - n = \frac{t^2}{2}
        \]

        Now note that
        \[
            \lim_{n \to \infty} b_n = b \implies \lim_{n \to \infty} \left(1 + \frac{b_n}{n}\right)^n = e^b
        \]

        Thus we see that
        \[
            \lim_{n \to \infty} M_{U_n}(t) = \lim_{n \to \infty} \left[M_{Z_1}\left(\frac{t}{\sqrt{n}}\right)\right]^n = e^{\frac{t^2}{2}}
        \]

        Thus we see that $\lim\limits_{n \to \infty} M_{U_n}(t) = M_{N(0,1)}(t)$ and thus by the uniqueness of MGFs we see that $U_n \overset{d}{\to} N(0,1)$.
    \end{proof}

    \bigskip

    The \textbf{Central Limit Theorem} states that the sample mean from any probability distribution (as long as mean and variance are \uline{finite}) will have an approximate normal distribution, if the sample is sufficiently large.

    \bigskip

    ``Large \( n \)'' means \( n \geq 30 \) in general, but in some cases may even be much less.

    \bigskip

    The larger the sample size, the more nearly normally distributed is the population of all possible sample means.

    \bigskip

    For fairly symmetric distributions, \( n > 15 \) will be sufficient.

    \newpage

    \subsection{Normal Approximation to the Binomial}

    The normal distribution (continuous) can be used to approximate binomial (discrete) probabilities when there is a very large number of trials and when \(np\) and \(n(1 - p)\) are both large (\(np \geq 5\) and \(n(1 - p) \geq 5\)).

    \bigskip

    \begin{theorem}
        \textbf{Normal Approximation to the Binomial}\\
        If \(Y \sim \text{Bin}(n,p)\), then \(Y \approx N(\mu = np, \sigma^2 = npq)\)
    \end{theorem}

    \begin{proof}
        The justification for the normal approximation to the binomial is based on the Central Limit Theorem.
    \end{proof}

    \bigskip
    
    To improve the accuracy of the approximation, we usually use a correction factor, \uline{called continuity correction}, to take into account that the binomial random variable is discrete while the normal is continuous.

    \bigskip

    The basic idea is to treat the discrete value \(b\) as the continuous interval from \(b - 0.5\) to \(b + 0.5\) giving the following adjustments:

    \begin{itemize}
        \item \(P(Y = b) = P(b - 0.5 \leq Y \leq b + 0.5)\)
        \item \(P(Y \leq b) = P(Y \leq b + 0.5)\)
        \item \(P(b \leq Y) = P(b - 0.5 \leq Y)\)
    \end{itemize}

    \newpage

    \section{Chapter 8 \textemdash{} Estimation}

    If \( Y_1, Y_2, \ldots, Y_n \overset{i.i.d.}{\sim} N(\mu, 1) \), where \(\mu\) is unknown, then how do we estimate \(\mu\)?
    
    \bigskip

    An \textcolor{red}{estimator} is a rule, often expressed as a formula, that tells how to calculate the value of an estimate based on the measurements contained in a sample.
    
    \bigskip

    Let \( Y_1, Y_2, \ldots, Y_n \) be i.i.d. with pdf/pmf \( f = f(\cdot|\theta) = f_\theta \), where the parameter \(\theta\) is unknown ($f$ depends on $\theta$). We want to find an estimate for the unknown parameter \(\theta\).

    \subsection{Point Estimation}
    \begin{definition}
        A \textbf{point estimator} $\hat{\theta}$ of the parameter \(\theta\) is a function of the underlying random variables and so it is a random variable with a distribution function.
    \end{definition}

    \bigskip

    A point estimate of \(\theta\) is a function of the sample \(Y_1, Y_2, \ldots, Y_n\) only. That is if $y_1, \ldots, y_n$ is the observed sample then $\hat{\theta} = U(y_1, \ldots, y_n)$ is a number.

    \bigskip

    For example a distribution with mean $\mu$ and variance $\sigma^2$ we have that $\overline{Y}$ is a point estimator for $\mu$ and $S^2$ is a point estimator for $\sigma^2$.

    \bigskip

    We say that a point estimator $\hat{\theta}$ is "good" if it has the following desirable properties:
    \begin{enumerate}
        \item Unbiased
        \item Consistent
        \item Minimum Variance
        \item Has a known probability distribution
    \end{enumerate}
    
    \begin{definition}
        Let $\hat{\theta}$ be a point estimator for a parameter $\theta$. Then $\hat{\theta}$ is an \textbf{unbiased estimator} if $E(\hat{\theta}) = \theta$. If $E(\hat{\theta}) \neq \theta$, then $\hat{\theta}$ is said to be \textbf{biased}.
    \end{definition}
    Note that $\hat{\theta}$ is a random variable whereas $\theta$ is a constant.

    \bigskip

    \begin{definition}
        The \textbf{bias} of a point estimator $\hat{\theta}$ is given by $B(\hat{\theta}) = E(\hat{\theta}) - \theta$.
    \end{definition}
    We see that a point estimator $\hat{\theta}$ is unbiased w.r.t. $\theta$ if $B(\hat{\theta}) = 0$.

    \bigskip
        
    \begin{definition}
        The \textbf{mean square error} of a point estimator $\hat{\theta}$ is
        \[
            MSE(\hat{\theta}) = E\left[ (\hat{\theta} - \theta)^2 \right]
        \]
    \end{definition}

    \begin{theorem}
        $MSE(\hat{\theta}) = V(\hat{\theta}) + \left[B(\hat{\theta})\right]^2$
    \end{theorem}
    It can be seen that if the estimator is unbiased then $MSE(\hat{\theta}) = V(\hat{\theta})$.

    \bigskip

    \textbf{Examples of Well-Known Unbiased Estimators}

    Let $Y_1, Y_2, \ldots, Y_n$ be a random sample with $E(Y_i) = \mu_1$ and $X_1, X_2, \ldots, X_n$ be a random sample with $E(X_i) = \mu_2$. Then:

    \begin{align*}
        E\left(\overline{Y}\right) &= \mu_1 \quad \text{and} \quad E\left(\overline{X}\right) = \mu_2\\
        E\left(\overline{Y} - \overline{X}\right) &= \mu_1 - \mu_2
    \end{align*}
    
    Let $Y \sim \text{Bin}(n, p_1)$ and $X \sim \text{Bin}(n, p_2)$. Then $\hat{p}_1 = \frac{Y}{n}$ is the proportion of successes in the sample.
    
    \begin{align*}
        E\left(\hat{p}_1\right) &= E\left(\frac{Y}{n}\right) = \frac{1}{n} E(Y) = \frac{np_1}{n} = p_1\\
        E\left(\hat{p}_2\right) &= E\left(\frac{X}{n}\right) = \frac{1}{n} E(X) = \frac{np_2}{n} = p_2\\
        E\left(\hat{p}_1 - \hat{p}_2\right) &= p_1 - p_2
    \end{align*}

    \newpage

    \subsection{Evaluating the Goodness of an Estimator}

    \begin{definition}
        Let \(\sigma^2_{\hat{\theta}}\) be the variance of the sampling distribution of the estimator \(\hat{\theta}\)

        (i.e. \(V(\hat{\theta}) = \sigma^2_{\hat{\theta}}\)), then \(\sqrt{V(\hat{\theta})} = \sqrt{\sigma^2_{\hat{\theta}}} = \sigma_{\hat{\theta}}\) is called the \textbf{standard error} of the estimator. That is, the \textbf{standard error} of \(\hat{\theta} =\) the standard deviation of \(\hat{\theta}\).
    \end{definition}
    Note that we call it the standard error because it comes from the sampling process.

    \bigskip

    \begin{definition}
        The \textbf{error of estimation} \(\varepsilon\) is the distance between an estimator and its target parameter. That is, \(\varepsilon = |\hat{\theta} - \theta|\).
    \end{definition}

    \bigskip

    \begin{definition}
        The \textbf{2-standard-error bound} on the error of estimation is given by $2SE(\hat{\theta}) = 2\sigma_{\hat{\theta}}$.
    \end{definition}
    To place a 2-standard-error bound on the error of estimation is to find the probability that the error of estimation is within 2 standard errors of the estimator. That is to find $P(|\varepsilon| < 2SE(\hat{\theta})) = P(|\varepsilon| < 2 \sigma_{\hat{\theta}})$.

    \newpage

    \subsection{Confidence Intervals}

    An alternative to reporting a single value for the parameter being estimated is to calculate and report an entire interval of plausible values; i.e., a \textbf{confidence interval} (CI).

    \textbf{Properties of the CI:}

    \begin{itemize}
        \item It contains true parameter \(\theta\) with high probability
        \item It is relatively narrow
    \end{itemize}

    The \textbf{lower} and \textbf{upper} endpoints of a CI are called the \textbf{lower} and \textbf{upper confidence limits}.

    The probability that a CI will enclose \(\theta\) is called the \textbf{confidence coefficient} or \textbf{confidence level}, denoted by \(1 - \alpha\).

    \bigskip

    \begin{definition}
        A \(100(1 - \alpha)\%\) \textbf{confidence interval} for a parameter \(\theta\) is a random interval \([\hat{\theta}_L, \hat{\theta}_U]\) such that 
        \[
        P\left( \hat{\theta}_L \leq \theta \leq \hat{\theta}_U \right) = 1 - \alpha,
        \]
        regardless of the value of \(\theta\).
    \end{definition}
        
    Properties of the CI:
        
    \begin{itemize}
        \item Here, \(\hat{\theta}_L\) and \(\hat{\theta}_U\) are functions of \(Y_i\)'s only.
        \item \textbf{Two-sided CI}: \([\hat{\theta}_L, \hat{\theta}_U]\)
        \item \textbf{Lower one-sided CI}: \([\hat{\theta}_L, \infty)\)
        \item \textbf{Upper one-sided CI}: \((-\infty, \hat{\theta}_U]\)
        \item The confidence level is denoted by \(100(1 - \alpha)\%\). The most common confidence levels are 90\%, 95\%, and 99\%.
        \item The higher the confidence level, the more strongly we believe that the true value of the parameter being estimated lies within the interval.
    \end{itemize}

    \bigskip

    One way of getting CI is through the \textbf{Pivotal Method}:

    \begin{itemize}
        \item We need to find a random variable, called a \textbf{pivot}, that is typically a function of the estimator of \(\theta\) satisfying the following:
        \begin{enumerate}
            \item It depends on and only on \(Y_1, \ldots, Y_n\) and \(\theta\): \(Q(y_1, \ldots, y_n \mid \theta)\).
            \item Its probability distribution (pdf/cdf) does not depend on \(\theta\) or any other unknown parameter.
        \end{enumerate}
        \item Use the distribution of the pivot to find the cutting points \(a\) and \(b\):
        \[
        P(a \leq Q \leq b) = 1 - \alpha.
        \]
        \item Write down \(P(a \leq Q \leq b)\) to get \(P(L(y_1, \ldots, y_n) \leq \theta \leq U(y_1, \ldots, y_n))\), if you can.
        \item Thus, \((1-\alpha)100\%\) CI for \(\theta\) is: \([\hat{\theta}_L, \hat{\theta}_U] = [L(y_1, \ldots, y_n), U(y_1, \ldots, y_n)]\).
    \end{itemize}

    Note that the Pivot is not unique.

    \bigskip

    In general we use 3 methods to show a pivot's distribution does not depend on $\theta$:
    \begin{enumerate}
        \item Use the CDF of the pivot and show that it does not depend on $\theta$.
        \item Use the MGF of the pivot and the uniqueness of MGFs to show that its corresponding pdf does not depend on $\theta$.
        \item Directly show that the pivot has a similar distribution as a known distribution.
    \end{enumerate}

    An example of method III:\\
    Let $Y$ be an observation from an exponential distritbution. Then $\frac{Y}{\theta}$ is a pivot.
    \begin{proof}
        Note that $Y \sim \text{Exp}(\theta)$ and thus $Y \sim \text{Gamma}(1, \theta)$. Now note that as
        \[
        \phi \text{Gamma}(\alpha, \beta) = \text{Gamma}(\alpha, \phi\beta)
        \]
        we see that
        \[
        \frac{Y}{\theta} \sim \frac{1}{\theta}\text{Gamma}(1,\theta) = \text{Gamma}(1,\frac{1}{\theta}\theta) = \text{Gamma}(1, 1) \sim \text{Exp(1)}
        \]
        and thus does not depend on $\theta$.
    \end{proof}

    \bigskip

    \textbf{How to find the cutting points:}\\
    Suppose we have a pivot $U$ of $Y_1, \ldots, Y_n$ and $\theta$ and we want to find cutting points $a$ and $b$ such that $P(a \leq U \leq b) = 1 - \alpha$. We then see that $P(U \leq a) + P(b \leq U) = \alpha$. Thus there may be many solutions for cutting points. Thus as a convention we find $a$ and $b$ such that $P(U \leq a) = \frac{\alpha}{2}$ or $P(a \leq U) = 1 - \frac{\alpha}{2}$ and $P(b \leq U) = \frac{\alpha}{2}$. Thus it can be seen the cutting points are $U_{\frac{\alpha}{2}}$ and $U_{1 - \frac{\alpha}{2}}$. This shortcut can be especially helpful when the pivot is distributed similar to a known distribution.

    \newpage

    \subsection{Large-Sample Confidence Intervals}

    If the target parameter \(\theta\) is \(\mu, p, \mu_1 - \mu_2\) or \(p_1 - p_2\), then for large samples by CLT
        
    \[
        Z = \frac{\hat{\theta} - \theta}{\sigma_{\hat{\theta}}} \sim N(0, 1)
    \]
        
    The distribution of \(Z\) doesn’t depend on \(\theta\). So it is \textbf{pivotal}.
        
    The pivotal method can be employed to develop confidence intervals for the target parameter \(\theta\).

    \begin{theorem}
        It can be seen that the confidence interval for any $\theta$ on a large sample is given by
    \[
        \left[\hat{\theta} - Z_{\frac{\alpha}{2}} \sigma_{\hat{\theta}} , \hat{\theta} + Z_{\frac{\alpha}{2}} \sigma_{\hat{\theta}}\right] = \hat{\theta} \mp Z_{\frac{\alpha}{2}} \sigma_{\hat{\theta}}
    \]
    \end{theorem}

    \begin{definition}
        We denote $Z_{\frac{\alpha}{2}} \sigma_{\hat{\theta}}$ as the \textbf{margin of error} of $\hat{\theta}$. It is a bound on the error of estimation.
    \end{definition}

    \bigskip

    Consider $\frac{\overline{Y} - \mu}{\sigma / \sqrt{n}} \sim N(0, 1)$. Is this a pivotal quantity for $\mu$?\\
    Indeed the distribution does not depend on $\mu$ however it may depend $\sigma$. If $\sigma$ is known then it is pivotal however if it is unknown then it is not pivotal. To this end we develop a pivotal quantity with the $t$-distribution.

    \bigskip

    \begin{theorem}
        Let \[
            T = \frac{\overline{Y} - \mu}{s/\sqrt{n}} \sim t_{n-1}
        \]

        Then $T$ is a pivotal quantity for $\mu$. Moreover the $100(1-\alpha)\%$ CI for $\mu$ is given by

        \[
        \left[ \overline{Y} - t_{\alpha/2} \frac{S}{\sqrt{n}}, \overline{Y} + t_{\alpha/2} \frac{S}{\sqrt{n}} \right]
        \]
    \end{theorem}

    \bigskip

    Suppose we want to compare between two datasets. Then we study the difference in their means or proportions. We can use the $t$-distribution to develop a pivotal quantity for the difference in means or proportions.\\
    Let $Y_1, Y_2, \ldots, Y_n \overset{\text{i.i.d.}}{\sim} N(\mu_1, \sigma_1^2)$ and $X_1, X_2, \ldots, X_n \overset{\text{i.i.d.}}{\sim} N(\mu_2, \sigma_2^2)$.\\
    Assume the two popoulations are independent and that $\sigma_1^2 = \sigma_2^2 = \sigma^2$, where $\sigma$ is unknown.\\
    Then we have $\overline{Y} - \overline{X} \sim N\left(\mu_1 - \mu_2, \sigma^2 \left(\frac{1}{n_1} + \frac{1}{n_2}\right)\right)$.
    \begin{definition}
        We define the \textbf{pooled variance} as
    \[
        S_p^2 = \frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{(n_1-1) + (n_2-1)}
    \]
    where $S_1^2$ and $S_2^2$ are the sample variances.
    \end{definition}

    \bigskip

    \begin{theorem}
        $T$ defined as below is a pivotal quantity for $\mu_1 - \mu_2$:
        \[
            T = \frac{Z}{\sqrt{W/(n_1 + n_2 - 2)}} = \frac{\overline{Y} - \overline{X} - (\mu_1 - \mu_2)}{S_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim t(n_1 + n_2 - 2)
        \]
        The $100(1-\alpha)\%$ CI for $\mu_1 - \mu_2$ is given by
        \[
            \left[ 
            \overline{Y} - \overline{X} - t_{\alpha/2} S_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}, \ 
            \overline{Y} - \overline{X} + t_{\alpha/2} S_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}} 
            \right] = \overline{Y} - \overline{X} \mp t_{\alpha/2} S_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}
        \]
    \end{theorem}

    \begin{proof}
        We see that
        \[
            Z = \frac{\overline{Y} - \overline{X} - (\mu_1 - \mu_2)}{\sigma \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim N(0, 1)
        \]

        and

        \[
            W = \frac{n_1 + n_2 - 2}{\sigma^2} S_p^2 \sim \chi^2_{n_1 + n_2 - 2} 
        \]

        Then we have
        \[
            T = \frac{Z}{\sqrt{W/(n_1 + n_2 - 2)}} = \frac{\frac{\overline{Y} - \overline{X} - (\mu_1 - \mu_2)}{\sigma \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}}{\sqrt{\frac{\frac{n_1 + n_2 - 2}{\sigma^2} S_p^2}{(n_1+n_2-2)}}} = \frac{\frac{\overline{Y} - \overline{X} - (\mu_1 - \mu_2)}{\sigma \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}}{\sqrt{\frac{S_p^2}{\sigma^2}}} = \frac{\frac{\overline{Y} - \overline{X} - (\mu_1 - \mu_2)}{\sigma \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}}{\frac{S_p}{\sigma}}
        \]
        It follows then
        \[
            T = \frac{\overline{Y} - \overline{X} - (\mu_1 - \mu_2)}{S_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim t(n_1 + n_2 - 2)
        \]
    \end{proof}

    \bigskip

    Consider the case of finding a confidence interval for $\sigma^2$.

    \begin{theorem}
        Let \(Y_1, \ldots, Y_n \overset{\text{iid}}{\sim} N(\mu, \sigma^2), \text{ where } \mu \text{ and } \sigma^2 \text{ are unknown.}\)

        $(n - 1)\frac{S^2}{\sigma^2} \sim \chi^2_{n-1}$ is a pivotal quantity for \(\sigma^2\).

        \(100(1-\alpha)\%\) CI for \(\sigma^2\) is:

        \[
            \left[ \frac{(n-1)S^2}{\chi^2_{\alpha/2}}, \frac{(n-1)S^2}{\chi^2_{1-\alpha/2}} \right]
        \]
    \end{theorem}

    \begin{proof}
        As $(n - 1)\frac{S^2}{\sigma^2} \sim \chi^2_{n-1}$ we have
        \[
            P\left( \chi^2_{\alpha/2} \leq (n - 1)\frac{S^2}{\sigma^2} \leq \chi^2_{1-\alpha/2} \right) = 1 - \alpha
        \]
        It can then be seen that
        \[
            P\left(\frac{(n - 1)S^2}{\chi^2_{\alpha/2}} \leq \sigma^2 \leq \frac{(n - 1)S^2}{\chi^2_{1-\alpha/2}} \right) = 1 - \alpha
        \]
    \end{proof}

    \newpage

    \section{Chapter 9 \textemdash{} Properties of Point Estimators and Methods of Estimation}

    \subsection{Relative Efficiency}
    
    \begin{definition}
        Given two unbiased estimators $\hat{\theta}_1$ and $\hat{\theta}_2$ of a parameter $\theta$, with variances $V(\hat{\theta}_1)$ and $V(\hat{\theta}_2)$, respectively, then the \textbf{efficiency} of $\hat{\theta}_1$ relative to $\hat{\theta}_2$, denoted $\text{eff}(\hat{\theta}_1, \hat{\theta}_2)$, is defined to be the ratio
            
        \[
            eff(\hat{\theta}_1, \hat{\theta}_2) = \frac{V(\hat{\theta}_2)}{V(\hat{\theta}_1)}
        \]
    \end{definition}

    \begin{definition}
        If $\hat{\theta}_1$ and $\hat{\theta}_2$ denote two unbiased estimators for the same parameter $\theta$, then we say that $\hat{\theta}_1$ \textbf{is relatively more efficient} than $\hat{\theta}_2$ if $V(\hat{\theta}_2) > V(\hat{\theta}_1)$.
    \end{definition}

    It can be seen that \(\text{eff}(\hat{\theta}_1, \hat{\theta}_2) > 1 \implies\) $\hat{\theta}_1$ is relatively more efficient than $\hat{\theta}_2$.\\
    Likewise it can be seen that \(\text{eff}(\hat{\theta}_1, \hat{\theta}_2) < 1 \implies\) $\hat{\theta}_2$ is relatively more efficient than $\hat{\theta}_1$.\\
    It can also be seen that \(\text{eff}(\hat{\theta}_1, \hat{\theta}_2) = 1 \implies\) $\hat{\theta}_1$ and $\hat{\theta}_2$ are equally efficient.

    \bigskip

    In practice when given two unbiased estimators we choose the one with the smaller variance.\\
    If \(\text{eff}(\hat{\theta}_1, \hat{\theta}_2) > 1 \implies \text{choose } \hat{\theta}_1.\)\\
    If \(\text{eff}(\hat{\theta}_1, \hat{\theta}_2) < 1 \implies \text{choose } \hat{\theta}_2.\)

    \newpage

    \subsection{Consistency}

    \begin{definition}
        The estimator $\hat{\theta}_n$ is said to be a \textbf{consistent} estimator of $\theta$ if $\hat{\theta}_n$ converges in probability to $\theta$. That is, for any positive number $\epsilon$,
        
        \[
            \lim_{n \to \infty} P\left( |\hat{\theta}_n - \theta| \leq \epsilon \right) = 1
        \]
        
        or, equivalently,
        
        \[
            \lim_{n \to \infty} P\left( |\hat{\theta}_n - \theta| > \epsilon \right) = 0
        \]
    \end{definition}
        
    \begin{theorem} \textbf{Variance Rule}\\
        An \underline{unbiased} estimator $\hat{\theta}_n$ for $\theta$ is consistent if
        
        \[
            \lim_{n \to \infty} V\left( \hat{\theta}_n \right) = 0
        \]
    \end{theorem}

    \begin{theorem} \textbf{Weak Law of Large Numbers (WLLN)}\\
        Let $Y_1, \ldots, Y_n$ be independent random variables with expected value $\mu$ and variance $\sigma^2$. Then the sample mean
        
        \[
            \overline{Y}_n = \frac{1}{n} \sum_{i=1}^{n} Y_i \xrightarrow{p} \mu
        \]
        
        \begin{itemize}
            \item The WLLN states that as the number of observations in a sample increases, the sample mean approaches the population mean.
            \item In simpler terms, the more data you collect, the closer your average will be to the true average of the entire population.
            \item In fact, \(\frac{1}{n} \sum\limits_{i=1}^{n} g(Y_i) \xrightarrow{p} E[g(Y)]\).
        \end{itemize}
        
    \end{theorem}

    \begin{proof}
        Follows directly from using Chebyshev's Inequality
    \end{proof}

    \begin{theorem}
        \textbf{Continuous Mapping Theorem:} Let \(g\) be a function that is continuous at \(\theta\). If \(\hat{\theta}_n \xrightarrow{p} \theta\), then \(g(\hat{\theta}_n) \xrightarrow{p} g(\theta)\).
    \end{theorem}

    \newpage

    \subsection{Sufficiency}

    \begin{definition}
        Let \( Y_1, Y_2, \ldots, Y_n \) denote a random sample from a probability distribution with unknown parameter \(\theta\). (\( Y_1, Y_2, \ldots, Y_n \sim f(y|\theta)\))\\
        Then the statistic \( U = U(Y_1, Y_2, \ldots, Y_n) \) is said to be \textbf{sufficient} for \(\theta\) if the conditional distribution of \( Y_1, Y_2, \ldots, Y_n \), given \( U \), does not depend on \(\theta\). That is $f(y_1, y_2, \ldots, y_n | U)$ does not require $\theta$.
    \end{definition}

    \bigskip
        
    \textbf{Interpretation of a Sufficient Statistic:}
    \begin{itemize}
        \item All information about the value of the parameter \(\theta\) is contained in the value of the sufficient statistic.
        \item A statistic is sufficient if no other statistic that can be calculated from the same sample provides any additional information as to the value of the parameter.
        \item A statistic is sufficient if the sample from which it is calculated gives no additional information than does the statistic.
    \end{itemize}
        
    \begin{itemize}
        \item Sufficient estimators are important in constructing "good (efficient)" estimators: MVUE.
        \item \textbf{How to find a sufficient statistic for \(\theta\)?}
    \end{itemize}

    \begin{definition}
        Let \(y_1, y_2, \ldots, y_n\) be sample observations taken on corresponding random variables \(Y_1, Y_2, \ldots, Y_n\) whose distribution depends on a parameter \(\theta\). Then, if \(Y_1, Y_2, \ldots, Y_n\) are discrete (continuous) random variables, the \textbf{likelihood of the sample}, \(L(\theta) = L(y_1, y_2, \ldots, y_n \mid \theta)\), is defined to be the joint probability (density) of \(y_1, y_2, \ldots, y_n\). That is,
        
        \begin{itemize}
            \item \textbf{Discrete case:}
            \[
            L(\theta) = L(y_1, \ldots, y_n \mid \theta) = p(y_1, \ldots, y_n) = p(y_1 \mid \theta) \times \cdots \times p(y_n \mid \theta) = \prod_{i=1}^{n} p(y_i \mid \theta)
            \]
        
            \item \textbf{Continuous case:}
            \[
            L(\theta) = L(y_1, \ldots, y_n \mid \theta) = f(y_1, \ldots, y_n) = f(y_1 \mid \theta) \times \cdots \times f(y_n \mid \theta) = \prod_{i=1}^{n} f(y_i \mid \theta)
            \]
        \end{itemize}
    \end{definition}

    \begin{theorem} \textbf{Factorization Theorem}\\
        Let \( U \) be a statistic based on the random sample \( Y_1, Y_2, \ldots, Y_n \). Then \( U \) is a sufficient statistic for the estimation of a parameter \(\theta\) if and only if the likelihood \( L(\theta) = L(y_1, y_2, \ldots, y_n \mid \theta) \) can be factored into two nonnegative functions,
        
        \[
        L(y_1, y_2, \ldots, y_n \mid \theta) = g(U, \theta) \times h(y_1, y_2, \ldots, y_n),
        \]
        
        where \( g(u, \theta) \) is a function only of \( u \) and \(\theta\) and \( h(y_1, y_2, \ldots, y_n) \) is not a function of \(\theta\).
    \end{theorem}

    \newpage

    \subsection{Completeness and Uniqueness}

    \begin{itemize}
        \item Let \( Y_1, \ldots, Y_n \overset{\text{i.i.d.}}{\sim} f_\theta \)
        \item \textbf{Notation:} \( g(U) = 0 \text{ a.e.} \) means \( P(\{U : g(U) \neq 0\}) = 0 \)
    \end{itemize}
    
    \begin{definition}
    A statistic \( U = U(Y_1, \ldots, Y_n) \) is said to be \textbf{complete} if and only if for every function \( g(U) \) such that
    
    \[
        E(g(U)) = 0 \text{ for all } \theta \implies g(U) = 0 \text{ a.e.}
    \]
    \end{definition}
    
    \begin{theorem}
        Under completeness, any unbiased estimator of is unique.
    \end{theorem}

    \begin{proof}
        If \( E(g_1(U)) = \theta \) and \( E(g_2(U)) = \theta \), then \( g_1(U) \) and \( g_2(U) \) are unbiased estimators of \(\theta\). Then it follows that

        \[
            E(g_1(U) - g_2(U)) = \theta - \theta = 0
        \]

        By completeness, \(g_1(U) - g_2(U) = 0 \text{ a.e.} \Rightarrow g_1(U) = g_2(U) \text{ a.e.}\)
    \end{proof}

    \textbf{How to find a complete and a sufficient statistic?}

    \textbf{Two methods:}

    \begin{itemize}
        \item \textbf{Method I:} \textit{(not required)}
        \begin{enumerate}
            \item Obtain a sufficient statistic by the factorization theorem.
            \item Check completeness via definition: \(E(g(U)) = 0\) for all \(\theta\) implies that \(g(U) = 0\) a.e.
            \item It is often challenging to directly verify completeness from the definition.
        \end{enumerate}
        \item \textbf{Method II:} Using the \underline{Exponential Family} property, as described on the next page.
    \end{itemize}

    \newpage

    \subsection{Exponential Family}

    \begin{definition} \textbf{Exponential Family}\\
        Consider the pdf/pmf \( f(\cdot \mid \theta) \).
        
        \begin{enumerate}
            \item \( f(y \mid \theta) \) is said to be a member of the \textbf{exponential class} if it can be written in the form:
        
            \[
            f(y \mid \theta) = e^{p(\theta)K(y) + q(\theta) + S(y)}, \quad \text{if } a < y < b,
            \]
            
            \item \( f(y \mid \theta) \) is said to be a \textbf{regular} member of the exponential class (family) if, in addition to (1) above:
            \begin{enumerate}
                \item \(a\) and \(b\) are free of \(\theta\)
                \item \(p(\theta)\) and \(K(y)\) are non-trivial \([\iff p'(\theta) \neq 0 \text{ and } K'(y) \neq 0]\)
                \item \(S(y)\) is continuous
            \end{enumerate}
        \end{enumerate}
        
        \end{definition}

        \begin{theorem}
            Consider the pdf/pmf \(f(y \mid \theta)\), where \(\theta \in \Omega\). If \(f(y \mid \theta)\) is a regular member of the exponential class, then \(U = \sum\limits_{i=1}^{n} K(y_i)\) is \underline{sufficient} and \underline{complete}.
        \end{theorem}

    \newpage

    \subsection{The Rao-Blackwell Theorem}

    \begin{definition}
        Let \( Y_1, \ldots, Y_n \) be a random sample of size \( n \) from \( f(y \mid \theta) \). An estimator \( T = T(Y_1, Y_2, \ldots, Y_n) \) is called a \textbf{minimum variance unbiased estimator (MVUE)} of the parameter \(\theta\) if \( T \) is unbiased, that is, \( E(T) = \theta \), and if the variance of \( T \) is less than or equal to the variance of every other unbiased estimator of \(\theta\).
    \end{definition}

    \begin{theorem} \textbf{The Rao-Blackwell Theorem}\\
        Let \(\hat{\theta}\) be an \underline{unbiased} estimator for \(\theta\) such that \(V(\hat{\theta}) < \infty\). If \(U\) is a \underline{sufficient statistic} for \(\theta\), define \(\hat{\theta}^* = E(\hat{\theta} \mid U)\). Then, for all \(\theta\),
        
        \[
            E(\hat{\theta}^*) = \theta \quad \text{and} \quad V(\hat{\theta}^*) \leq V(\hat{\theta}).
        \]

        That is, $\theta^* = E(\text{unbiased}|\text{sufficient})$ is a MVUE.
    \end{theorem}

    \bigskip

    Rao-Blackwell Theorem gives a procedure to improve two given estimators as follows:

    \begin{enumerate}
        \item Start with an unbiased estimator \(\hat{\theta}\) for a parameter \(\theta\) and the sufficient statistic \(U\) obtained through the factorization criterion.
    
        \item Then \(\hat{\theta}^* = E(\hat{\theta} \mid U)\) is \textbf{"better"} than \(\hat{\theta}\) [since it is unbiased as \(\hat{\theta}\) and has variance less than that of \(\hat{\theta}\)] and is \textbf{"better"} than \(U\) [since it is a function of the sufficient statistic and unbiased].
    \end{enumerate}

    \newpage

    \subsection{Unique Minimum Variance Unbiased Estimator (UMVUE)}

    \begin{theorem} \textbf{Lehmann-Scheffé Theorem}\\
        Suppose \( Y_1, Y_2, \ldots, Y_n \) are i.i.d. with pdf \( f(y \mid \theta) \). If \( U_1 = U_1(Y_1, \ldots, Y_n) \) is an \underline{unbiased} estimator of \(\theta\) and \( U_2 = U_2(Y_1, \ldots, Y_n) \) is \underline{sufficient} and \underline{complete}, then \( E(U_1 \mid U_2) \) is the \underline{Unique Minimum Variance Unbiased Estimator (UMVUE)} of \(\theta\).

        \bigskip

        That is \(E(\text{unbiased}|\text{sufficient and complete}) = \text{UMVUE}\).
    \end{theorem}

    \bigskip

    It is unique because of the completeness property.\\
    It has minimum variance by Rao-Blackwell Theorem.

    \bigskip

    We have two procedures to find the UMVUE:

    \begin{itemize}
        \item \textbf{Procedure 1:} Two steps
        \begin{enumerate}
            \item Get \(U_1\) and \(U_2\) such that \(U_1\) is an unbiased estimator and \(U_2\) is sufficient and complete.
            \item Then \(E(U_1 \mid U_2)\) is the UMVUE.
        \end{enumerate}
        
        \item \textbf{Procedure 2:} Two steps
        \begin{enumerate}
            \item Find a complete and sufficient statistic \(U\).
            \item Find \(\phi(U)\) such that \(E(\phi(U)) = \theta\). Then \(\phi(U)\) is the UMVUE.
        \end{enumerate}
    \end{itemize}

    % \newpage

    % \subsection{Cramer-Rao Theorem}

    % \newpage

    % \subsection{How to find Estimators}

    % \newpage

    % \section{Chapter 10 \textemdash{} Hypothesis Testing}

    % \subsection{Elements of a Statistical Test}

    % \newpage

    % \subsection{Neyman-Pearson Lemma}

    % \newpage

    % \subsection{Likelihood Ratio Tests}
\end{document}